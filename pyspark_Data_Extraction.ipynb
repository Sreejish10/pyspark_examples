{"cells":[{"cell_type":"markdown","source":["#read csv file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b855147c-b5d7-42f3-bfdf-05e869b1ca90","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_csv = spark.read.format(\"csv\").load(\"/FileStore/global2ksample.csv\", header=True, inferSchema=True)\n#to limit and see first 3 rows\ndf_csv.limit(3).display()\n#filter a record\ndf_csv.where(\"priority_rank=='2'\").display()\n#display all columns\ndf_csv.columns\n#change column type using select for only specific columns\nfrom pyspark.sql.types import IntegerType\ndf_csv_new = df_csv.select(df_csv.priority_rank.cast(IntegerType()))\ndf_csv_new.printSchema()\n\n#change column using with to modify in same dataframe\ndf_csv = df_csv.withColumn(\"priority_rank\",df_csv[\"priority_rank\"].cast(IntegerType()))\ndf_csv.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25754664-9a5b-4a71-bee6-a652f902c545","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#read Json file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e1c3f76-4b41-41e8-be66-8fa5f62c3c07","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#for multiline json output use this\ndf_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/test_response.json\")\ndf_json.limit(3).display()\n#to view all schema\ndf_json.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e611996c-463a-4236-881b-8832998b1886","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#create partition by date"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a9b8a41-68e6-46ee-871d-6dae451b2fba","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#adding new column\nfrom pyspark.sql.functions import *\ndf_csv = df_csv.withColumn(\"created_date\",current_date())\n\ndf_csv.display()\n\n\n\n#creating partition by year\ndf_csv.write.format('delta') \\\n            .mode('overwrite') \\\n            .partitionBy('created_date').saveAsTable(\"partition_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04dbde7f-7fcf-40d8-9753-0653eb3707c1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#verify the table\n%fs ls"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"09986345-f038-4a84-bc6b-90b64e561c09","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark_Data_Extraction","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2867250878420035}},"nbformat":4,"nbformat_minor":0}
