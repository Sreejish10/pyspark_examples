{"cells":[{"cell_type":"markdown","source":["#read csv file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b855147c-b5d7-42f3-bfdf-05e869b1ca90","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_csv = spark.read.format(\"csv\").load(\"/FileStore/global2ksample.csv\", header=True, inferSchema=True)\n#to limit and see first 3 rows\ndf_csv.limit(3).display()\n#filter a record\ndf_csv.where(\"priority_rank=='2'\").display()\n#display all columns\ndf_csv.columns\n#change column type using select for only specific columns\nfrom pyspark.sql.types import IntegerType\ndf_csv_new = df_csv.select(df_csv.priority_rank.cast(IntegerType()))\ndf_csv_new.printSchema()\n\n#change column using with to modify in same dataframe\ndf_csv = df_csv.withColumn(\"priority_rank\",df_csv[\"priority_rank\"].cast(IntegerType()))\ndf_csv.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25754664-9a5b-4a71-bee6-a652f902c545","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#read Json file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e1c3f76-4b41-41e8-be66-8fa5f62c3c07","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#for multiline json output use this\ndf_json = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/test_response.json\")\ndf_json.limit(3).display()\n#to view all schema\ndf_json.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e611996c-463a-4236-881b-8832998b1886","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["create partition by date"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a9b8a41-68e6-46ee-871d-6dae451b2fba","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#adding new column\nfrom pyspark.sql.functions import *\ndf_csv = df_csv.withColumn(\"created_date\",current_date())\ndf_csv = df_csv.withColumn(\"YEAR\",date_format(\"created_date\",'yyyy'))\n\ndf_csv.display()\ndf_csv.printSchema()\n\n\n\n#creating partition by year\ndf_csv.write.format('delta') \\\n            .mode('overwrite') \\\n            .partitionBy('YEAR').saveAsTable(\"test_partition_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04dbde7f-7fcf-40d8-9753-0653eb3707c1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs ls user/hive/warehouse/test_partition_table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"09986345-f038-4a84-bc6b-90b64e561c09","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from test_partition_table where YEAR='2022'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"b4d95229-c20a-4d85-95a1-7a633f91d5e9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Handle datafile with variable columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86872212-9050-4a92-9d88-b57be42c5fbd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#lets create a file with random columns\ndbutils.fs.put(\"/variable_column.csv\",\"\"\"1,sree\n1,sreejish,data,engineer\n1,sreejsh,data\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8482093f-120e-4ad2-bc61-029af076c976","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#read it as text file so all columns comes under one field\ndf1 = spark.read.text(\"/variable_column.csv\")\ndf1.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3bddd8d9-09c5-4cd9-8fa0-00404ec9eaad","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#create a split columns using split function\nfrom pyspark.sql.functions import split\ndf1 = df1.withColumn(\"splittable_column\",split(\"value\",\",\")).drop(\"value\")#dropping value columns\ndf1.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1043e0c9-a260-417c-8218-bd8f60268495","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#find maximum number of columns, use size function\nfrom pyspark.sql.functions import size,max\n#truncate False used to show all, get max size of number of col\ndf1.select(max(size(\"splittable_column\"))).show(truncate=False)\n#loop through and create col and ad value\nfor i in range(df1.select(max(size(\"splittable_column\"))).collect()[0][0]):\n    df1 = df1.withColumn(\"col\"+str(i),df1['splittable_column'][i])\ndf1.drop(\"splittable_column\").display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c0c3156-8282-497e-94f3-68873614ddd6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#skip first few rows from file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2f3c3a2-8b68-4cce-b7e4-e6edfac66bcf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dbutils.fs.put(\"/skiprows.csv\",\"\"\"line1\nline2\nline3\nuser,role,exp\nsree,databricks,2\nsree,Azure,3\nsree,GCP,1\"\"\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8f0b552-10a6-486f-901f-54f565cb161d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd = sc.textFile(\"/skiprows.csv\")\n#zipWithIndex function to create a sequence, split the extra rows using the below function\nrdd_final=rdd.zipWithIndex().filter(lambda a:a[1]>2).map(lambda a:a[0].split(\",\"))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2713ef8-819d-45bb-8357-747a28c9748f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["col= rdd_final.collect()[0] #get only columns\n#skip first row\nskipline=rdd_final.first()\n\nrdd_final.filter(lambda a:a!=skipline).toDF(col).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc9a9422-7e07-4b5e-8a3b-c93df65a5c53","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#remove duplicates from Dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51e67d00-2db1-4e95-a003-dbe8789a3b6b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_dup = spark.read.csv(\"/FileStore/global2ksample.csv\", header=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98468f21-b993-4ece-a30b-ca309235db40","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf_dup.orderBy(col(\"priority_rank\").desc()).dropDuplicates([\"country\"]).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af785ea6-ccb7-467d-9cbd-a3de77ee9fd5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark_Data_Extraction","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":667523952417394,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":2867250878420035}},"nbformat":4,"nbformat_minor":0}
